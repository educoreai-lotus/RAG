# ğŸ”„ ×–×¨×™××ª BATCH TRIGGER - ×”×¡×‘×¨ ××¤×•×¨×˜

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×
1. [×“×¨×›×™ ×”×¤×¢×œ×”](#×“×¨×›×™-×”×¤×¢×œ×”)
2. [×–×¨×™××” ××œ××”](#×–×¨×™××”-××œ××”)
3. [×§×‘×¦×™× ××¢×•×¨×‘×™×](#×§×‘×¦×™×-××¢×•×¨×‘×™×)
4. [×¤×™×¨×•×˜ ×©×œ×‘×™×](#×¤×™×¨×•×˜-×©×œ×‘×™×)

---

## ğŸš€ ×“×¨×›×™ ×”×¤×¢×œ×”

### 1. **×”×¤×¢×œ×” ××•×˜×•××˜×™×ª (Scheduled)**
- **×§×•×‘×¥:** `BACKEND/src/jobs/scheduledSync.js`
- **××ª×™:** ×œ×¤×™ ×œ×•×— ×–×× ×™× (Cron Schedule)
- **×‘×¨×™×¨×ª ××—×“×œ:** ×›×œ ×™×•× ×‘×©×¢×” 2:00 ×‘×œ×™×œ×” (`0 2 * * *`)
- **×”×’×“×¨×”:** `BATCH_SYNC_SCHEDULE` env var
- **×”×¤×¢×œ×”:** `startScheduledSync()` × ×§×¨× ×‘-`BACKEND/src/index.js` ×‘×¢×ª ×”×¤×¢×œ×ª ×”×©×¨×ª

### 2. **×”×¤×¢×œ×” ×™×“× ×™×ª (Manual Trigger)**
- **Endpoint:** `POST /admin/batch-sync/trigger`
- **×§×•×‘×¥:** `BACKEND/src/routes/admin.routes.js`
- **××ª×™:** ×œ×¤×™ ×‘×§×©×” ××”×× ×”×œ
- **×¤×•× ×§×¦×™×”:** `runBatchSync()`

### 3. **×”×¤×¢×œ×” ×‘×¢×ª ×”×¤×¢×œ×” (On Startup)**
- **×ª× ××™:** `BATCH_SYNC_ON_STARTUP=true`
- **××ª×™:** 5 ×©× ×™×•×ª ××—×¨×™ ×”×¤×¢×œ×ª ×”×©×¨×ª
- **×§×•×‘×¥:** `BACKEND/src/jobs/scheduledSync.js` (×©×•×¨×” 140-149)

---

## ğŸ”„ ×–×¨×™××” ××œ××”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH TRIGGER FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. TRIGGER (×”×¤×¢×œ×”)
   â”‚
   â”œâ”€â–º Scheduled (Cron) â”€â”€â–º scheduledSync.js â”€â”€â–º runBatchSync()
   â”‚
   â”œâ”€â–º Manual (API) â”€â”€â”€â”€â”€â”€â”€â–º admin.routes.js â”€â”€â”€â”€â–º runBatchSync()
   â”‚
   â””â”€â–º On Startup â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º scheduledSync.js â”€â”€â”€â”€â–º runBatchSync()
   
2. runBatchSync()
   â”‚
   â””â”€â–º syncAllServices() â”€â”€â–º batchSyncService.js
       â”‚
       â”œâ”€â–º getServicesToSync() â”€â”€â–º ×¨×©×™××ª ×©×™×¨×•×ª×™×
       â”‚
       â””â”€â–º syncService(serviceName) â”€â”€â–º ×œ×›×œ ×©×™×¨×•×ª
           â”‚
           â”œâ”€â–º Pagination Loop (×¢××•×“×™×)
           â”‚   â”‚
           â”‚   â”œâ”€â–º batchSync() â”€â”€â–º coordinator.client.js
           â”‚   â”‚   â”‚
           â”‚   â”‚   â””â”€â–º Coordinator (gRPC) â”€â”€â–º Microservice
           â”‚   â”‚
           â”‚   â”œâ”€â–º processCoordinatorResponse()
           â”‚   â”‚
           â”‚   â””â”€â–º Extract data from envelope_json
           â”‚
           â””â”€â–º updateDataStore(serviceName, allData)
               â”‚
               â”œâ”€â–º buildResponseEnvelope()
               â”‚
               â””â”€â–º batchHandler.handle()
                   â”‚
                   â”œâ”€â–º schemaLoader.getSchema()
                   â”œâ”€â–º dataExtractor.extractItems()
                   â”œâ”€â–º processParallel()
                   â”‚   â”‚
                   â”‚   â”œâ”€â–º processChunk()
                   â”‚   â”‚   â”‚
                   â”‚   â”‚   â”œâ”€â–º vectorizer.generateBatch()
                   â”‚   â”‚   â”‚
                   â”‚   â”‚   â””â”€â–º storage.store() â”€â”€â–º vector_embeddings
                   â”‚   â”‚       â”‚
                   â”‚   â”‚       â””â”€â–º kgBuilder.buildFromContent() (optional)
                   â”‚   â”‚
                   â”‚   â””â”€â–º Parallel processing (5 workers, 50 items/chunk)
                   â”‚
                   â””â”€â–º Return results
```

---

## ğŸ“ ×§×‘×¦×™× ××¢×•×¨×‘×™×

### 1. **scheduledSync.js** - Scheduler
**××™×§×•×:** `BACKEND/src/jobs/scheduledSync.js`

**×ª×¤×§×™×“:**
- × ×™×”×•×œ ×œ×•×— ×”×–×× ×™× (Cron)
- ×”×¤×¢×œ×ª batch sync ××•×˜×•××˜×™×ª
- ×× ×™×¢×ª ×”×¤×¢×œ×•×ª ×›×¤×•×œ×•×ª (`isRunning` flag)

**×¤×•× ×§×¦×™×•×ª ×¢×™×§×¨×™×•×ª:**
```javascript
runBatchSync(options)        // ×”×¤×¢×œ×ª sync
startScheduledSync()         // ×”×ª×—×œ×ª scheduler
stopScheduledSync()          // ×¢×¦×™×¨×ª scheduler
getSchedulerStatus()         // ×¡×˜×˜×•×¡ scheduler
```

**×”×’×“×¨×•×ª ×¡×‘×™×‘×”:**
- `BATCH_SYNC_ENABLED` - ×”×¤×¢×œ×”/×›×™×‘×•×™ (default: true)
- `BATCH_SYNC_SCHEDULE` - ×œ×•×— ×–×× ×™× (default: `0 2 * * *`)
- `BATCH_SYNC_ON_STARTUP` - ×”×¤×¢×œ×” ×‘×¢×ª ×”×¤×¢×œ×” (default: false)
- `BATCH_SYNC_TIMEZONE` - ××–×•×¨ ×–××Ÿ (default: UTC)

---

### 2. **admin.routes.js** - Manual Trigger
**××™×§×•×:** `BACKEND/src/routes/admin.routes.js`

**Endpoints:**
```javascript
POST /admin/batch-sync/trigger  // ×”×¤×¢×œ×” ×™×“× ×™×ª
GET  /admin/batch-sync/services  // ×¨×©×™××ª ×©×™×¨×•×ª×™×
```

**×–×¨×™××”:**
```
POST /admin/batch-sync/trigger
  â””â”€â–º runBatchSync()
      â””â”€â–º syncAllServices()
```

---

### 3. **batchSyncService.js** - Core Service
**××™×§×•×:** `BACKEND/src/services/batchSyncService.js`

**×¤×•× ×§×¦×™×•×ª ×¢×™×§×¨×™×•×ª:**

#### `getServicesToSync()`
**×ª×¤×§×™×“:** ×§×‘×œ×ª ×¨×©×™××ª ×©×™×¨×•×ª×™× ×œ×¡× ×›×¨×•×Ÿ

**×¢×“×™×¤×•×™×•×ª:**
1. `BATCH_SYNC_SERVICES` env var (×¢×§×™×¤×” ×™×“× ×™×ª)
2. Coordinator's list (××§×•×¨ ×”×××ª)
3. Fallback list (×× Coordinator × ×›×©×œ)

#### `syncService(serviceName, options)`
**×ª×¤×§×™×“:** ×¡× ×›×¨×•×Ÿ ×©×™×¨×•×ª ×‘×•×“×“ ×¢× pagination

**×–×¨×™××”:**
```javascript
1. ×‘×“×™×§×ª BATCH_SYNC_ENABLED
2. Pagination Loop:
   â”œâ”€â–º batchSync() â”€â”€â–º Coordinator (gRPC)
   â”œâ”€â–º processCoordinatorResponse()
   â”œâ”€â–º Extract data from envelope_json
   â””â”€â–º Accumulate allData
3. updateDataStore(serviceName, allData)
```

**×¤×¨××˜×¨×™×:**
- `serviceName` - ×©× ×”×©×™×¨×•×ª
- `options.syncType` - ×¡×•×’ sync ('batch', 'daily', 'incremental')
- `options.since` - ×ª××¨×™×š ISO ×œ×¡× ×›×¨×•×Ÿ ××™× ×§×¨×× ×˜×œ×™

#### `syncAllServices(options)`
**×ª×¤×§×™×“:** ×¡× ×›×¨×•×Ÿ ×›×œ ×”×©×™×¨×•×ª×™×

**×–×¨×™××”:**
```javascript
1. getServicesToSync() â”€â”€â–º ×¨×©×™××ª ×©×™×¨×•×ª×™×
2. For each service:
   â””â”€â–º syncService(serviceName, options)
3. Aggregate results
```

#### `updateDataStore(serviceName, data)` â­ NEW
**×ª×¤×§×™×“:** ×©××™×¨×ª × ×ª×•× ×™× ×‘-vector DB

**×–×¨×™××”:**
```javascript
1. buildResponseEnvelope(data)
2. batchHandler.handle({
     source_service: serviceName,
     tenant_id: tenantId,
     response_envelope: responseEnvelope
   })
```

#### `buildResponseEnvelope(responseData)` â­ NEW
**×ª×¤×§×™×“:** ×‘× ×™×™×ª response envelope ×‘×¤×•×¨××˜ ×”× ×“×¨×©

**×ª××™×›×” ×‘×¤×•×¨××˜×™×:**
- Format 1: `{ data: { items: [...] } }`
- Format 2: `{ data: [...] }` (array)
- Format 3: `{ successfulResult: { data: [...] } }`
- Format 4: `[...]` (raw array)

---

### 4. **coordinator.client.js** - gRPC Client
**××™×§×•×:** `BACKEND/src/clients/coordinator.client.js`

**×¤×•× ×§×¦×™×”:** `batchSync(params)`

**×¤×¨××˜×¨×™×:**
```javascript
{
  target_service: 'service-name',  // â­ CRITICAL
  sync_type: 'batch',                // â­ CRITICAL
  page: 1,
  limit: 1000,
  since: '2024-01-01T00:00:00Z'     // optional
}
```

**×ª×¤×§×™×“:**
- ×§×¨×™××” ×œ-Coordinator ×“×¨×š gRPC
- ×§×‘×œ×ª × ×ª×•× ×™× ××”-Microservice
- ×”×—×–×¨×ª response ×¢× `envelope_json`

---

### 5. **batchHandler.js** - Data Processing
**××™×§×•×:** `BACKEND/src/handlers/batchHandler.js`

**×¤×•× ×§×¦×™×”:** `handle(input)`

**Input:**
```javascript
{
  source_service: 'service-name',
  tenant_id: 'tenant-id',
  response_envelope: {
    data: { items: [...] },
    metadata: { page, total, has_more }
  }
}
```

**×–×¨×™××”:**
```javascript
1. schemaLoader.getSchema(source_service)
2. dataExtractor.extractItems(response_envelope, schema)
3. processParallel(extractedItems, tenantId, schema)
   â”‚
   â”œâ”€â–º Split into chunks (50 items/chunk)
   â”‚
   â””â”€â–º Process chunks in parallel (5 workers)
       â”‚
       â””â”€â–º processChunk(items, tenantId, schema)
           â”‚
           â”œâ”€â–º dataExtractor.buildContent() â”€â”€â–º contents[]
           â”œâ”€â–º vectorizer.generateBatch() â”€â”€â”€â–º embeddings[]
           â”‚
           â””â”€â–º For each item:
               â”œâ”€â–º storage.store() â”€â”€â–º vector_embeddings
               â””â”€â–º kgBuilder.buildFromContent() (optional, background)
```

---

### 6. **storage.js** - Vector Storage
**××™×§×•×:** `BACKEND/src/core/storage.js`

**×¤×•× ×§×¦×™×”:** `store(item, content, embedding, tenantId, schema)`

**×ª×¤×§×™×“:**
- ×©××™×¨×” ×‘-`vector_embeddings` table
- ×™×¦×™×¨×ª/×¢×“×›×•×Ÿ embeddings
- ×™×¦×™×¨×ª/×¢×“×›×•×Ÿ microservice record

---

### 7. **kgBuilder.service.js** â­ NEW - Knowledge Graph
**××™×§×•×:** `BACKEND/src/services/kgBuilder.service.js`

**×¤×•× ×§×¦×™×”:** `buildFromContent(contentId, contentText, metadata, tenantId)`

**×ª×¤×§×™×“:**
- ×™×¦×™×¨×ª nodes ×‘-`knowledge_graph_nodes`
- ×™×¦×™×¨×ª edges ×‘-`knowledge_graph_edges`
- ××¦×™××ª ×ª×•×›×Ÿ ×§×©×•×¨ (semantic similarity)

**×–×¨×™××”:**
```javascript
1. createContentNode() â”€â”€â–º knowledge_graph_nodes
2. findRelatedContent() â”€â”€â–º Semantic search
3. createRelationshipEdges() â”€â”€â–º knowledge_graph_edges
```

---

## ğŸ” ×¤×™×¨×•×˜ ×©×œ×‘×™×

### ×©×œ×‘ 1: TRIGGER (×”×¤×¢×œ×”)

#### ×. Scheduled Trigger
```javascript
// BACKEND/src/index.js (×©×•×¨×” 688)
startScheduledSync();

// BACKEND/src/jobs/scheduledSync.js
export function startScheduledSync() {
  scheduledTask = cron.schedule(BATCH_SYNC_SCHEDULE, async () => {
    await runBatchSync();
  });
}
```

#### ×‘. Manual Trigger
```javascript
// POST /admin/batch-sync/trigger
router.post('/batch-sync/trigger', async (req, res) => {
  const result = await runBatchSync();
  res.json({ success: true, result });
});
```

---

### ×©×œ×‘ 2: runBatchSync()

```javascript
// BACKEND/src/jobs/scheduledSync.js
export async function runBatchSync(options = {}) {
  // ×‘×“×™×§×ª isRunning flag
  if (isRunning) return { success: false, reason: 'already_running' };
  
  // ×‘×“×™×§×ª BATCH_SYNC_ENABLED
  if (!BATCH_SYNC_ENABLED) return { success: false, reason: 'disabled' };
  
  isRunning = true;
  
  // ×§×¨×™××” ×œ-syncAllServices
  const result = await syncAllServices({
    syncType: 'daily',
    ...options
  });
  
  isRunning = false;
  return result;
}
```

---

### ×©×œ×‘ 3: syncAllServices()

```javascript
// BACKEND/src/services/batchSyncService.js
export async function syncAllServices(options = {}) {
  // 1. ×§×‘×œ×ª ×¨×©×™××ª ×©×™×¨×•×ª×™×
  const services = await getServicesToSync();
  
  // 2. ×¡× ×›×¨×•×Ÿ ×›×œ ×©×™×¨×•×ª (×‘×œ×ª×™ ×ª×œ×•×™)
  for (const serviceName of services) {
    try {
      const result = await syncService(serviceName, options);
      results.push(result);
    } catch (error) {
      // ×©×’×™××” ×‘×©×™×¨×•×ª ××—×“ ×œ× ×¢×•×¦×¨×ª ××ª ×”×©××¨
      results.push({ success: false, error: error.message });
    }
  }
  
  // 3. ×¡×™×›×•× ×ª×•×¦××•×ª
  return {
    success: failedServices === 0,
    services: results,
    totalItems,
    totalErrors
  };
}
```

---

### ×©×œ×‘ 4: syncService() - Pagination

```javascript
// BACKEND/src/services/batchSyncService.js
export async function syncService(serviceName, options = {}) {
  let page = 1;
  let hasMore = true;
  const allData = [];
  
  // Pagination Loop
  while (hasMore) {
    // 1. ×§×¨×™××” ×œ-Coordinator
    const response = await batchSync({
      target_service: serviceName,
      sync_type: 'batch',
      page,
      limit: BATCH_SYNC_LIMIT
    });
    
    // 2. ×¢×™×‘×•×“ ×ª×’×•×‘×”
    const processed = processCoordinatorResponse(response);
    
    // 3. ×—×™×œ×•×¥ × ×ª×•× ×™×
    const envelope = JSON.parse(response.envelope_json);
    const pageData = envelope.payload?.data || [];
    
    // 4. ××™×¡×•×£ × ×ª×•× ×™×
    allData.push(...pageData);
    
    // 5. ×‘×“×™×§×ª ×¢××•×“×™× × ×•×¡×¤×™×
    const hasMoreFlag = response.normalized_fields?.has_more;
    if (hasMoreFlag === 'true' || hasMoreFlag === true) {
      page++;
    } else if (pageData.length < BATCH_SYNC_LIMIT) {
      hasMore = false;
    } else {
      page++;
    }
  }
  
  // 6. ×©××™×¨×ª ×›×œ ×”× ×ª×•× ×™×
  if (allData.length > 0) {
    await updateDataStore(serviceName, allData);
  }
}
```

---

### ×©×œ×‘ 5: updateDataStore() â­ NEW

```javascript
// BACKEND/src/services/batchSyncService.js
async function updateDataStore(serviceName, data) {
  // 1. ×‘× ×™×™×ª response envelope
  const responseEnvelope = buildResponseEnvelope(data);
  
  // 2. ×§×¨×™××” ×œ-batchHandler
  const batchHandler = await import('../handlers/batchHandler.js');
  const tenantId = process.env.DEFAULT_TENANT_ID || 'default-tenant';
  
  await batchHandler.default.handle({
    source_service: serviceName,
    tenant_id: tenantId,
    response_envelope: responseEnvelope
  });
}
```

---

### ×©×œ×‘ 6: batchHandler.handle()

```javascript
// BACKEND/src/handlers/batchHandler.js
async handle(input) {
  const { source_service, tenant_id, response_envelope } = input;
  
  // 1. ×˜×¢×™× ×ª schema
  const schema = schemaLoader.getSchema(source_service);
  
  // 2. ×—×™×œ×•×¥ items
  const extractedItems = dataExtractor.extractItems(response_envelope, schema);
  
  // 3. ×¢×™×‘×•×“ ××§×‘×™×œ×™
  const results = await this.processParallel(extractedItems, tenant_id, schema);
  
  return {
    success: true,
    processed: extractedItems.length,
    successful: results.filter(r => r.success).length,
    failed: results.filter(r => !r.success).length
  };
}
```

---

### ×©×œ×‘ 7: processParallel()

```javascript
// BACKEND/src/handlers/batchHandler.js
async processParallel(items, tenantId, schema) {
  const BATCH_SIZE = 50;
  const WORKERS = 5;
  
  // 1. ×—×œ×•×§×” ×œ-chunks
  const chunks = [];
  for (let i = 0; i < items.length; i += BATCH_SIZE) {
    chunks.push(items.slice(i, i + BATCH_SIZE));
  }
  
  // 2. ×¢×™×‘×•×“ ××§×‘×™×œ×™ (5 workers)
  for (let i = 0; i < chunks.length; i += WORKERS) {
    const batch = chunks.slice(i, i + WORKERS);
    const promises = batch.map(chunk => 
      this.processChunk(chunk, tenantId, schema)
    );
    const batchResults = await Promise.all(promises);
    results.push(...batchResults.flat());
  }
  
  return results;
}
```

---

### ×©×œ×‘ 8: processChunk()

```javascript
// BACKEND/src/handlers/batchHandler.js
async processChunk(items, tenantId, schema) {
  // 1. ×‘× ×™×™×ª ×ª×•×›×Ÿ
  const contents = items.map(item => 
    dataExtractor.buildContent(item, schema)
  );
  
  // 2. ×™×¦×™×¨×ª embeddings (batch)
  const embeddings = await vectorizer.generateBatch(contents);
  
  // 3. ×©××™×¨×” ×œ×›×œ item
  for (let i = 0; i < items.length; i++) {
    // ×©××™×¨×” ×‘-vector_embeddings
    await storage.store(
      items[i],
      contents[i],
      embeddings[i],
      tenantId,
      schema
    );
    
    // â­ NEW: ×‘× ×™×™×ª Knowledge Graph (background, optional)
    kgBuilder.buildFromContent(
      contentId,
      contents[i],
      { source_service: schema.service_name, batch_processed: true },
      tenantId
    ).catch(kgError => {
      // ×œ× × ×›×©×œ ×× KG × ×›×©×œ
      logger.debug('[Batch] KG build failed (non-critical)');
    });
  }
}
```

---

## âš™ï¸ ×”×’×“×¨×•×ª ×¡×‘×™×‘×”

```bash
# ×”×¤×¢×œ×”/×›×™×‘×•×™
BATCH_SYNC_ENABLED=true

# ×œ×•×— ×–×× ×™× (Cron)
BATCH_SYNC_SCHEDULE=0 2 * * *          # ×›×œ ×™×•× ×‘×©×¢×” 2:00
BATCH_SYNC_TIMEZONE=UTC

# ×”×’×“×¨×•×ª pagination
BATCH_SYNC_LIMIT=1000                  # ×¤×¨×™×˜×™× ×œ×¢××•×“

# ×¨×©×™××ª ×©×™×¨×•×ª×™× (×¢×§×™×¤×” ×™×“× ×™×ª)
BATCH_SYNC_SERVICES=service1,service2,service3

# ×”×¤×¢×œ×” ×‘×¢×ª ×”×¤×¢×œ×”
BATCH_SYNC_ON_STARTUP=false

# Tenant ID (×œ×©××™×¨×”)
DEFAULT_TENANT_ID=default-tenant
```

---

## ğŸ“Š ×“×™××’×¨××ª ×–×¨×™××” ××¤×•×¨×˜×ª

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH SYNC FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[TRIGGER]
    â”‚
    â”œâ”€â–º Scheduled (Cron) â”€â”€â”
    â”œâ”€â–º Manual (API) â”€â”€â”€â”€â”€â”€â”¤
    â””â”€â–º On Startup â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
[runBatchSync()]
    â”‚
    â”œâ”€â–º Check: isRunning? â”€â”€â–º Skip if yes
    â”œâ”€â–º Check: BATCH_SYNC_ENABLED? â”€â”€â–º Skip if no
    â”‚
    â–¼
[syncAllServices()]
    â”‚
    â”œâ”€â–º getServicesToSync()
    â”‚   â”œâ”€â–º BATCH_SYNC_SERVICES env var?
    â”‚   â”œâ”€â–º Coordinator.listServices()?
    â”‚   â””â”€â–º Fallback list
    â”‚
    â””â”€â–º For each service:
        â”‚
        â–¼
    [syncService(serviceName)]
        â”‚
        â”œâ”€â–º Pagination Loop:
        â”‚   â”‚
        â”‚   â”œâ”€â–º batchSync() â”€â”€â–º Coordinator (gRPC)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€â–º Microservice returns data
        â”‚   â”‚
        â”‚   â”œâ”€â–º processCoordinatorResponse()
        â”‚   â”œâ”€â–º Extract envelope_json
        â”‚   â””â”€â–º Accumulate allData
        â”‚
        â””â”€â–º updateDataStore(serviceName, allData)
            â”‚
            â”œâ”€â–º buildResponseEnvelope(data)
            â”‚
            â””â”€â–º batchHandler.handle()
                â”‚
                â”œâ”€â–º schemaLoader.getSchema()
                â”œâ”€â–º dataExtractor.extractItems()
                â”‚
                â””â”€â–º processParallel()
                    â”‚
                    â”œâ”€â–º Split into chunks (50 items)
                    â”‚
                    â””â”€â–º Process chunks (5 workers in parallel)
                        â”‚
                        â””â”€â–º processChunk()
                            â”‚
                            â”œâ”€â–º buildContent() â”€â”€â–º contents[]
                            â”œâ”€â–º generateBatch() â”€â”€â–º embeddings[]
                            â”‚
                            â””â”€â–º For each item:
                                â”œâ”€â–º storage.store() â”€â”€â–º vector_embeddings âœ…
                                â””â”€â–º kgBuilder.buildFromContent() â”€â”€â–º KG (optional) â­
```

---

## âœ… × ×§×•×“×•×ª ×—×©×•×‘×•×ª

1. **Pagination:** Batch sync ×ª×•××š ×‘-pagination ××•×˜×•××˜×™
2. **Error Handling:** ×©×’×™××” ×‘×©×™×¨×•×ª ××—×“ ×œ× ×¢×•×¦×¨×ª ××ª ×”×©××¨
3. **Parallel Processing:** ×¢×™×‘×•×“ ××§×‘×™×œ×™ (5 workers, 50 items/chunk)
4. **Knowledge Graph:** ×‘× ×™×™×ª KG ×”×™× ××•×¤×¦×™×•× ×œ×™×ª ×•×œ× ×—×•×¡××ª
5. **Tenant ID:** ××©×ª××© ×‘-`DEFAULT_TENANT_ID` ××• `'default-tenant'`
6. **Scheduling:** ×ª××™×›×” ×‘-Cron scheduling ×¢× node-cron
7. **Manual Trigger:** ××¤×©×¨×•×ª ×œ×”×¤×¢×œ×” ×™×“× ×™×ª ×“×¨×š API

---

## ğŸ”§ Debugging

### ×‘×“×™×§×ª ×¡×˜×˜×•×¡:
```bash
GET /health/batch-sync
```

### ×”×¤×¢×œ×” ×™×“× ×™×ª:
```bash
POST /admin/batch-sync/trigger
```

### ×¨×©×™××ª ×©×™×¨×•×ª×™×:
```bash
GET /admin/batch-sync/services
```

---

**× ×•×¦×¨:** $(date)
**×¢×•×“×›×Ÿ:** ×œ××—×¨ ×”×•×¡×¤×ª batch sync storage ×•-KG building

